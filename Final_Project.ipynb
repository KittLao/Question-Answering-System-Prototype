{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Final_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b06d23be5fae4de09966c816059f6dc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1d704732f5214405a6e4f324579c891a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_96d8468f164c4b629cf0a44d90b19cfb",
              "IPY_MODEL_1add8fd6ebca457fb90dc2e05c5bde0c"
            ]
          }
        },
        "1d704732f5214405a6e4f324579c891a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "96d8468f164c4b629cf0a44d90b19cfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_58cea2731a7c4a81b499c4c1ffe96c38",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_28ccd2da5581439a99558dc916c9e8be"
          }
        },
        "1add8fd6ebca457fb90dc2e05c5bde0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0401f954d1784713bfe80ef890faf6be",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 55/? [16:26&lt;00:00, 17.94s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dd967246509145dfaa4797b407b1c821"
          }
        },
        "58cea2731a7c4a81b499c4c1ffe96c38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "28ccd2da5581439a99558dc916c9e8be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0401f954d1784713bfe80ef890faf6be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dd967246509145dfaa4797b407b1c821": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ee4980a87d934e68aa47420277ba96fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4c567bb55ba947b7a536d64801e4f62f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_72d9d71730e640828f9405663c57c6af",
              "IPY_MODEL_967d6bfb673e4acfb73088cf2008181c"
            ]
          }
        },
        "4c567bb55ba947b7a536d64801e4f62f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "72d9d71730e640828f9405663c57c6af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_209e4d5170174ca1ab5498788fda2083",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_027640706660429a8ff2bdac11fb03a6"
          }
        },
        "967d6bfb673e4acfb73088cf2008181c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_33a207a4fabc4bbd9ef01b51327f1bef",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/? [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_47f45e74dfd8437888680af94bc243b5"
          }
        },
        "209e4d5170174ca1ab5498788fda2083": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "027640706660429a8ff2bdac11fb03a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "33a207a4fabc4bbd9ef01b51327f1bef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "47f45e74dfd8437888680af94bc243b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKjlulRnBVED"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeurgoMIru9c",
        "outputId": "f9800295-0ea9-4f7c-d89c-ac50c388e79a"
      },
      "source": [
        "import os\n",
        "import spacy # Text preprocessing\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "import math\n",
        "\n",
        "# Pytrch\n",
        "import torch\n",
        "import torch.nn as nn # Neural Net Layers module\n",
        "import torch.optim as optim # Optimizers module\n",
        "import torch.nn.functional as F # Functions module - activations, utilities like padding\n",
        "import numpy as np\n",
        "\n",
        "from tqdm.auto import tqdm # Add progress bar\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset # Preparing data in batches for pytorch training\n",
        "\n",
        "!pip install seqeval\n",
        "from seqeval.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Spacy\n",
        "from spacy.lemmatizer import Lemmatizer\n",
        "from spacy.lookups import Lookups\n",
        "\n",
        "# NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import math\n",
        "from itertools import chain\n",
        "\n",
        "# Gensim word embeddings\n",
        "import gensim\n",
        "import gensim.downloader\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "import pdb\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (0.17.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSrOrb1UsGvC"
      },
      "source": [
        "# **Word2Vec Wikipedia Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F465EjtcsZtD",
        "outputId": "2e7474c4-4862-421b-ed44-e07fdca0a845"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-14 01:21:10--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-12-14 01:21:10--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-12-14 01:21:10--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  2.01MB/s    in 6m 28s  \n",
            "\n",
            "2020-12-14 01:27:38 (2.12 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tW6cs0EXsf4S",
        "outputId": "db6ff865-34c8-4cb4-beef-5862e9955cb4"
      },
      "source": [
        "!unzip glove.6B.zip -d glove"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "replace glove/glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace glove/glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace glove/glove.6B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace glove/glove.6B.300d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5HuQ31msk8K"
      },
      "source": [
        "glove_input_file = 'glove/glove.6B.100d.txt'\n",
        "word2vec_output_file = 'glove.6B.100.txt.word2vec'\n",
        "glove2word2vec(glove_input_file, word2vec_output_file)\n",
        "w2v_weights = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpWH_XVltcil"
      },
      "source": [
        "# **Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX7L4go2tfgT"
      },
      "source": [
        "def squad_json_to_dataframe(input_file_path, record_path=['data','paragraphs','qas','answers'],\n",
        "                           verbose=1):\n",
        "    \"\"\"\n",
        "    input_file_path: path to the squad json file.\n",
        "    record_path: path to deepest level in json file default value is\n",
        "    ['data','paragraphs','qas','answers']\n",
        "    verbose: 0 to suppress it default is 1\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"Reading the json file\")    \n",
        "    file = json.loads(open(input_file_path).read())\n",
        "    if verbose:\n",
        "        print(\"processing...\")\n",
        "    # parsing different level's in the json file\n",
        "    js = pd.io.json.json_normalize(file , record_path )\n",
        "    m = pd.io.json.json_normalize(file, record_path[:-1] )\n",
        "    r = pd.io.json.json_normalize(file,record_path[:-2])\n",
        "    \n",
        "    #combining it into single dataframe\n",
        "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
        "#     ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n",
        "    m['context'] = idx\n",
        "#     js['q_idx'] = ndx\n",
        "    main = m[['id','question','context','answers']].set_index('id').reset_index()\n",
        "    main['c_id'] = main['context'].factorize()[0]\n",
        "    if verbose:\n",
        "        print(\"shape of the dataframe is {}\".format(main.shape))\n",
        "        print(\"Done\")\n",
        "    return main"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDj2a3BKtkwU"
      },
      "source": [
        "train_file_path = 'train-v1.1.json'\n",
        "dev_file_path = 'dev-v1.1.json'\n",
        "record_path = ['data','paragraphs','qas','answers'] # The only data necessary for our model."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJyz2nQut2ZJ",
        "outputId": "fe047268-2aaa-4421-b387-a80aedaea847"
      },
      "source": [
        "train_dataframe = squad_json_to_dataframe(input_file_path=train_file_path, record_path=record_path)\n",
        "dev_dataframe = squad_json_to_dataframe(input_file_path=dev_file_path,record_path=record_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading the json file\n",
            "processing...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "shape of the dataframe is (87599, 5)\n",
            "Done\n",
            "Reading the json file\n",
            "processing...\n",
            "shape of the dataframe is (10570, 5)\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SP7tSPNiulnU"
      },
      "source": [
        "# Traing and validation questions, context and answers\n",
        "train_questions, valid_questions, train_contexts, valid_contexts, train_answers, valid_answers = train_test_split(train_dataframe['question'], \n",
        "                                                train_dataframe['context'], \n",
        "                                                train_dataframe['answers'], train_size=.4, test_size=.1)\n",
        "# Test questions, context and answers\n",
        "test_questions, test_contexts, test_answers = dev_dataframe['question'], dev_dataframe['context'], dev_dataframe['answers']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qFyrLKLWcyw"
      },
      "source": [
        "def merge_data(contexts, paragraphs, answers):\n",
        "  dataset = []\n",
        "  for i in range(len(contexts)):\n",
        "    c, p, a = contexts[i], paragraphs[i], answers[i][0][\"text\"]\n",
        "    dataset.append((c, p, a))\n",
        "\n",
        "  return dataset\n",
        "\n",
        "train_data = merge_data(list(train_contexts), \n",
        "                        list(train_questions), \n",
        "                        list(train_answers))\n",
        "valid_data = merge_data(list(valid_contexts), \n",
        "                        list(valid_questions), \n",
        "                        list(valid_answers))\n",
        "test_data = merge_data(list(test_contexts), \n",
        "                        list(test_questions), \n",
        "                        list(test_answers))\n",
        "\n",
        "train_len = len(train_data)/10\n",
        "train_data = [train_data[int(i*train_len):int((i+1)*train_len)] for i in range(10)]\n",
        "valid_len = len(valid_data)/10\n",
        "valid_data = [valid_data[int(i*valid_len):int((i+1)*valid_len)] for i in range(10)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT9XYduiyuQt"
      },
      "source": [
        "# **Feature Builder Classes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_cIXgD61JWr"
      },
      "source": [
        "### **Word to Vector Sequencer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEaK9gk4yzcU"
      },
      "source": [
        "class W2VSequencer(object):\n",
        "  def __init__(self, w2v, dim=400):\n",
        "    self.w2v = w2v\n",
        "    self.nlp = spacy.load('en')\n",
        "    self.tokenizer = lambda text: [t.text for t in self.nlp(text)]\n",
        "\n",
        "    self.w2v.add(['<s>'], [np.zeros((dim,))])\n",
        "    self.w2v.add(['</s>'], [np.zeros((dim,))])\n",
        "    self.w2v.add(['<pad>'], [np.zeros((dim,))])\n",
        "    self.w2v.add(['<unk>'], [np.zeros((dim,))])\n",
        "\n",
        "    self.bos_index = self.w2v.vocab.get('<s>')\n",
        "    self.eos_index = self.w2v.vocab.get('</s>')\n",
        "    self.unk_index = self.w2v.vocab.get('<unk>')\n",
        "    self.pad_index = self.w2v.vocab.get('<pad>')\n",
        "\n",
        "  # Convert sentence to sequence of encodings based on the\n",
        "  # pre-trained embeddings.\n",
        "  def encode(self, text):\n",
        "    # Input will look like:\n",
        "    # [<s>, w1, w2, ..., wn, </s>]\n",
        "    sequence = [self.bos_index.index]\n",
        "    for token in self.tokenizer(text):\n",
        "\n",
        "      index = self.w2v.vocab.get(token, self.unk_index).index\n",
        "      sequence.append(index)\n",
        "    sequence.append(self.eos_index.index)\n",
        "\n",
        "    return sequence\n",
        "\n",
        "  # Pad sequences of tokenized paragraph. The number of sentences\n",
        "  # is variable, but each sentence has a max length. Returns the\n",
        "  # a tensor of padded sentences, and the original lengths of each\n",
        "  # sentence.\n",
        "  def padded_helper(self, sent_list):\n",
        "    lengths = [len(sequence) for sequence in sent_list]\n",
        "    # Max sentence length.\n",
        "    max_seq_len = max(lengths)\n",
        "    # 2 dimensional tensor.\n",
        "    tensor = torch.full((len(sent_list), max_seq_len), self.pad_index.index, dtype=torch.long)\n",
        "\n",
        "    for i, sequence in enumerate(sent_list):\n",
        "      for j, token in enumerate(sequence):\n",
        "        tensor[i][j] = token\n",
        "    \n",
        "    # tensor shape: [num_sentences, max_seq_len]\n",
        "    # lengths shape: [num_sentences, length_of_sentence]\n",
        "    return tensor, lengths\n",
        "\n",
        "  # Pad sequences of a batch size of tokenized paragraph. Each batch\n",
        "  # size will have the same number of paragraphs, each paragraph will\n",
        "  # have the same number of sentences, and each sentence will have the\n",
        "  # same number of words.\n",
        "  def create_padded_tensor_with_lengths(self, sequences):\n",
        "    # Given a list of sequences, pad all to the same length\n",
        "    tensor_list, lengths_list = [], []\n",
        "    for sequence in sequences:\n",
        "      tensor_i, lengths_i = self.padded_helper(sequence)\n",
        "      tensor_list.append(tensor_i)\n",
        "      lengths_list.append(lengths_i)\n",
        "    \n",
        "    # Max number of sentences.\n",
        "    max_dim_1 = max(len(p) for p in lengths_list)\n",
        "    # Max length of all sentence.\n",
        "    max_dim_2 = max(max(sent for sent in p) for p in lengths_list)\n",
        "    \n",
        "    # 3 dimensional tensor.\n",
        "    tensor = torch.full((len(sequences), max_dim_1, max_dim_2), \n",
        "                        self.pad_index.index, dtype=torch.long)\n",
        "    for i, p in enumerate(sequences):\n",
        "      for j, sent in enumerate(p):\n",
        "        for k, token in enumerate(sent):\n",
        "          tensor[i][j][k] = tensor_list[i][j][k]\n",
        "    \n",
        "    # tensor shape: [batch_size, num_sentences, max_seq_len]\n",
        "    # lengths shape: [batch_size, num_sentences, length_of_sentence]\n",
        "    return tensor, lengths_list\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84JFymeg1QBN"
      },
      "source": [
        "### **Feature Sequencer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfz1_iF51PRx"
      },
      "source": [
        "class FeatureSequencer(object):\n",
        "  def __init__(self, tokens, bos_token='<s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>'):\n",
        "    self.nlp = spacy.load('en')\n",
        "    self.pos2idx = {}\n",
        "    self.idx2pos = {}\n",
        "    self.tf = Counter()\n",
        "\n",
        "    self.pad_index = self.add_tag(pad_token)\n",
        "    self.unk_index = self.add_tag(unk_token)\n",
        "    self.bos_index = self.add_tag(bos_token)\n",
        "    self.eos_index = self.add_tag(eos_token)\n",
        "\n",
        "    for token in tokens:\n",
        "      self.tf[token] += 1\n",
        "    \n",
        "    self.tf_max = self.tf.most_common(1)[0][1]\n",
        "\n",
        "    self.tagger = lambda text: [(t.text, t.pos_) for t in self.nlp(text)]\n",
        "\n",
        "  def add_tag(self, tag):\n",
        "    self.pos2idx[tag] = new_index = len(self.pos2idx)\n",
        "    self.idx2pos[new_index] = tag\n",
        "\n",
        "    return new_index\n",
        "  \n",
        "  # Converts a sentence to a sequence of encodings. The encodings\n",
        "  # are the index of the word, and the term-frequency.\n",
        "  def encode(self, text, alpha=0.4):\n",
        "    sequence = [(self.bos_index, 1)]\n",
        "    for token, tag in self.tagger(text):\n",
        "      index = self.pos2idx.get(tag, self.add_tag(tag))\n",
        "      ntf = alpha + (1-alpha)*(self.tf[token]/self.tf_max)\n",
        "      sequence.append((index, ntf))\n",
        "    sequence.append((self.eos_index, 1))\n",
        "\n",
        "    return sequence\n",
        "\n",
        "  # Given a list of tokenized sentences, pad all sentence to the\n",
        "  # same length.\n",
        "  def padded_helper(self, sent_list):\n",
        "    # Max length of each sentence.\n",
        "    max_seq_len = max(len(sequence) for sequence in sent_list)\n",
        "    # 2 dimensional tensor.\n",
        "    tag_tensor = torch.full((len(sent_list), max_seq_len, 1), self.pad_index, dtype=torch.long)\n",
        "    tf_tensor = torch.full((len(sent_list), max_seq_len, 1, 1), 1., dtype=torch.float)\n",
        "\n",
        "    for i, sequence in enumerate(sent_list):\n",
        "      for j, (tag, ntf) in enumerate(sequence):\n",
        "        tag_tensor[i][j] = tag\n",
        "        tf_tensor[i][j] = ntf\n",
        "    \n",
        "    # tag_tensor_shape: [num_sentence, max_sent_len]\n",
        "    # tf_tensor: [num_sentence, max_sent_len]\n",
        "    return tag_tensor, tf_tensor\n",
        "  \n",
        "  # Given a batch of list of tokenized sentences, pad each sentence,\n",
        "  # then pad each list to have the same length.\n",
        "  def create_padded_tensor(self, sequences):\n",
        "    # Given a list of sequences, pad all to the same length\n",
        "    tag_list, tf_list = [], []\n",
        "    for sequence in sequences:\n",
        "      tag_i, tf_i = self.padded_helper(sequence)\n",
        "      tag_list.append(tag_i)\n",
        "      tf_list.append(tf_i)\n",
        "    \n",
        "    max_dim_1 = max(len(sequence) for sequence in sequences)\n",
        "    max_dim_2 = max(max(len(sent) for sent in sequence) for sequence in sequences)\n",
        "    \n",
        "    # 3 dimensional tensor.\n",
        "    tag_tensor = torch.full((len(sequences), max_dim_1, max_dim_2), self.pad_index, dtype=torch.long)\n",
        "    tf_tensor = torch.full((len(sequences), max_dim_1, max_dim_2), 1., dtype=torch.float)\n",
        "\n",
        "    for i, p in enumerate(sequences):\n",
        "      for j, sent in enumerate(p):\n",
        "        for k, token in enumerate(sent):\n",
        "          tag_tensor[i][j][k] = tag_list[i][j][k]\n",
        "          tf_tensor[i][j][k] = tf_list[i][j][k]\n",
        "    \n",
        "    # tag_tensor: [batch_size, num_sentence, max_sent_len]\n",
        "    # tag_tensor: [batch_size, num_sentence, max_sent_len]\n",
        "    return tag_tensor, tf_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Niby7yPs1Zzi"
      },
      "source": [
        "### **Extract Matching Sequences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uG43w9t1ZHh"
      },
      "source": [
        "class ExactMatchSequencer(object):\n",
        "  def __init__(self):\n",
        "    self.nlp = spacy.load('en')\n",
        "    # Tokenize paragraph into words.\n",
        "    self.tokenizer = lambda text: [t.text for t in self.nlp(text)]\n",
        "    self.lookups = Lookups()\n",
        "    self.lemm_obj = spacy.lemmatizer.Lemmatizer(self.lookups)\n",
        "    # Finds root of word.\n",
        "    self.lemmatizer = lambda token: self.lemm_obj.lookup(token)\n",
        "\n",
        "  # Given a paragraph and a question, determine if a paragraph\n",
        "  # could be matched to a questions. A sequence that is the\n",
        "  # same length of the number of words in the paragrpah. Each \n",
        "  # element is a 3-tuple containing an indication of whether\n",
        "  # any word in the question can be matched to a particular \n",
        "  # word in the paragraph. \n",
        "  # p_text = idividual sentence from parsed pararaph, q_text = question\n",
        "  def encode(self, p_text, q_text):\n",
        "    # Initial features from start token.\n",
        "    sequence = [(1, 1, 1)]\n",
        "    for p_i in self.tokenizer(p_text):\n",
        "      original, lowercase, lemma = 0, 0, 0\n",
        "      for q_j in self.tokenizer(q_text):\n",
        "        # Words exactly match.\n",
        "        original = 1 if p_i == q_j else original\n",
        "        # Lowercase words match.\n",
        "        lowercase = 1 if p_i.lower() == q_j.lower() else lowercase\n",
        "        # Root words match.\n",
        "        lemma = 1 if self.lemmatizer(p_i) == self.lemmatizer(q_j) else lemma\n",
        "      sequence.append((original, lowercase, lemma))\n",
        "    # End symbols from stop token.\n",
        "    sequence.append((1, 1, 1))\n",
        "\n",
        "    return sequence\n",
        "\n",
        "  def padded_helper(self, sent_list):\n",
        "    max_seq_len = max(len(sequence) for sequence in sent_list)\n",
        "    tensor = torch.full((len(sent_list), max_seq_len, 3), 0, dtype=torch.long)\n",
        "\n",
        "    for i, sequence in enumerate(sent_list):\n",
        "      for j, triple in enumerate(sequence):\n",
        "        tensor[i][j][:] = torch.tensor(list(triple))\n",
        "    \n",
        "    return tensor\n",
        "\n",
        "  def create_padded_tensor(self, sequences):\n",
        "    # Given a list of sequences, pad all to the same length\n",
        "    tensor_list = []\n",
        "    for sequence in sequences:\n",
        "      tensor_list.append(self.padded_helper(sequence))\n",
        "    \n",
        "    # Max number of sentences in paragraph for all in batch size.\n",
        "    max_dim_1 = max(len(sequence) for sequence in sequences)\n",
        "    # Max length of sentence for a paragraph.\n",
        "    max_dim_2 = max(max(len(sent) for sent in sequence) for sequence in sequences)\n",
        "    \n",
        "    tensor_tensor = torch.full((len(sequences), max_dim_1, max_dim_2, 3), 0, dtype=torch.long)\n",
        "\n",
        "    for i, p in enumerate(sequences):\n",
        "      for j, sent in enumerate(p):\n",
        "        for k, token in enumerate(sent):\n",
        "          tensor_tensor[i][j][k] = tensor_list[i][j][k]\n",
        "    \n",
        "    # tensor_tensor shape: [batch_size, num_sentences, num_words]\n",
        "    return tensor_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsoMB3YZm4Ea"
      },
      "source": [
        "###Answer Sequencer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZvcATOCm6nV"
      },
      "source": [
        "class AnswerSequencer(object):\n",
        "  def __init__(self):\n",
        "    self.nlp = spacy.load('en')\n",
        "    # Tokenize paragraph into words.\n",
        "    self.tokenizer = lambda text: [t.text for t in self.nlp(text)]\n",
        "\n",
        "  def encode(self, p_full_text, a_text):\n",
        "    sequence = []\n",
        "    a_tokens = self.tokenizer(a_text)\n",
        "    never_true = True\n",
        "    for i, sent in enumerate(nltk.sent_tokenize(p_full_text)):\n",
        "      sent_tokens = self.tokenizer(sent)\n",
        "      sequence.append([0]*(len(sent_tokens)+2))\n",
        "      if a_tokens[0] in sent_tokens:\n",
        "        indices = [i for i, x in enumerate(sent_tokens) if x==a_tokens[0]]\n",
        "        for index in indices:\n",
        "          sent_contains_answer = True\n",
        "          if len(a_tokens) > len(sent_tokens[index:]):\n",
        "            sent_contains_answer = False\n",
        "          for a_token, p_token in zip(a_tokens, sent_tokens[index:]):\n",
        "            if a_token != p_token:\n",
        "              sent_contains_answer = False\n",
        "          if sent_contains_answer:\n",
        "            sequence[i][index+1] = 1\n",
        "            sequence[i][index+len(a_tokens)] = 2\n",
        "            never_true = False\n",
        "    return sequence\n",
        "  \n",
        "  @staticmethod\n",
        "  def create_padded_tensor(sequences):\n",
        "    max_num_sent = max(len(sent_sequence) for sent_sequence in sequences)\n",
        "    max_seq_len = max(max(len(sequence) for sequence in sent_sequence) for sent_sequence in sequences)\n",
        "    tensor = torch.full((len(sequences), max_num_sent, max_seq_len), -1, dtype=torch.long)\n",
        "\n",
        "    for i, sent_sequence in enumerate(sequences):\n",
        "      for j, sequence in enumerate(sent_sequence):\n",
        "        for k, tag in enumerate(sequence):\n",
        "          tensor[i][j][k] = tag\n",
        "    \n",
        "    return tensor\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj8n58isSx1L"
      },
      "source": [
        "# **Preparing the sequencers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_G6GiiBS2DT"
      },
      "source": [
        "w2v_sequencer = W2VSequencer(w2v_weights, dim=100)\n",
        "combined_text = \" \".join(set(train_contexts))\n",
        "feature_sequencer = FeatureSequencer(combined_text)\n",
        "match_sequencer = ExactMatchSequencer()\n",
        "answer_sequencer = AnswerSequencer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MIuQJbtipwp"
      },
      "source": [
        "# **Dataset Class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvBR5MK5isjf"
      },
      "source": [
        "class QADataset(Dataset):\n",
        "  def __init__(self, data, w2v_sequencer, feature_sequencer, \n",
        "               answer_sequencer, exact_match_sequencer):\n",
        "    self.nlp = spacy.load(\"en\")\n",
        "    # [(paragraph, question, answer)]\n",
        "    # paragraph: str\n",
        "    # question: str\n",
        "    # answer: int\n",
        "    self.data = data\n",
        "\n",
        "    self.w2v_sequencer = w2v_sequencer\n",
        "    self.feature_sequencer = feature_sequencer\n",
        "    self.answer_sequencer = answer_sequencer\n",
        "    self.exact_match_sequencer = exact_match_sequencer\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    p, q, a = self.data[index]\n",
        "    # Encode paragraph features.\n",
        "    p_w2v, p_feature, p_match = [], [], []\n",
        "    for sent in nltk.sent_tokenize(p):\n",
        "      p_w2v.append(self.w2v_sequencer.encode(sent))\n",
        "      p_feature.append(self.feature_sequencer.encode(sent))\n",
        "      p_match.append(self.exact_match_sequencer.encode(sent, q))\n",
        "    \n",
        "    # Encode question.\n",
        "    q_w2v = self.w2v_sequencer.encode(q)\n",
        "    # Encode answer.\n",
        "    a_w2v = self.answer_sequencer.encode(p, a)\n",
        "\n",
        "    # Tokenized versions of the paragraph, question and answer\n",
        "    p_tokens = [[t.text for t in self.nlp(text)] for text in nltk.sent_tokenize(p)]\n",
        "    q_tokens = [t.text for t in self.nlp(q)]\n",
        "    a_tokens = [t.text for t in self.nlp(a)]\n",
        "\n",
        "    return p_w2v, p_feature, p_match, q_w2v, a_w2v, p_tokens, q_tokens, a_tokens\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4II016zRV2JK"
      },
      "source": [
        "# **Prepare Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG6sfRAPV1pz"
      },
      "source": [
        "train_dataset = [QADataset(train_data_single, w2v_sequencer, feature_sequencer, \n",
        "                          answer_sequencer, match_sequencer) for train_data_single in train_data[:-1]]\n",
        "valid_dataset = [QADataset(valid_data_single, w2v_sequencer, feature_sequencer, \n",
        "                          answer_sequencer, match_sequencer) for valid_data_single in valid_data[:-1]]\n",
        "test_dataset = QADataset(test_data, w2v_sequencer, feature_sequencer, \n",
        "                          answer_sequencer, match_sequencer)\n",
        "\n",
        "# A batch consists of 32 items. Each item is 3 paragraph features, \n",
        "# 1 for question and answer, and tokenized versions of paragraph, \n",
        "# question and answer.\n",
        "def prepare_batch(batch, w2v_sequencer, feature_sequencer, exact_match_sequencer):\n",
        "    # batch: [batch_len, (text, label)]\n",
        "    p_w2v, p_feature, p_match, q_w2v, a_w2v, p, q, a = zip(*batch)\n",
        "\n",
        "    # Pad all encodings.\n",
        "    p_w2v_tensor, p_w2v_len = w2v_sequencer.create_padded_tensor_with_lengths(p_w2v)\n",
        "    p_feature_tag, p_feature_tf = feature_sequencer.create_padded_tensor(p_feature)\n",
        "    p_match_tensor = exact_match_sequencer.create_padded_tensor(p_match)\n",
        "    q_w2v_tensor, q_w2v_len = w2v_sequencer.padded_helper(q_w2v)\n",
        "    a_w2v_tensor = AnswerSequencer.create_padded_tensor(a_w2v)\n",
        "\n",
        "    return (p_w2v_tensor, p_w2v_len, \n",
        "            p_feature_tag, p_feature_tf, \n",
        "            p_match_tensor, \n",
        "            q_w2v_tensor, q_w2v_len, \n",
        "            a_w2v_tensor, p, q, a)\n",
        "\n",
        "# Loader iterator: which produces a list of batches\n",
        "train_loader = [torch.utils.data.DataLoader(train_dataset_single, batch_size=16, \n",
        "                                            collate_fn=lambda batch: prepare_batch(batch, w2v_sequencer, feature_sequencer, match_sequencer))\n",
        "                                            for train_dataset_single in train_dataset]\n",
        "valid_loader = [torch.utils.data.DataLoader(valid_dataset_single, batch_size=16, \n",
        "                                           collate_fn=lambda batch: prepare_batch(batch, w2v_sequencer, feature_sequencer, match_sequencer), \n",
        "                                           shuffle=False) for valid_dataset_single in valid_dataset]\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
        "                                           batch_size=16, \n",
        "                                           collate_fn=lambda batch: prepare_batch(batch, w2v_sequencer, feature_sequencer, match_sequencer), \n",
        "                                           shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhovZajzYd9N"
      },
      "source": [
        "#**Model Class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyTSRQquJpm2"
      },
      "source": [
        "class net(nn.Module):\n",
        "  def __init__(self, embedding_dim=300, lstm_hidden_dim=100, w2v_weights=None, bidirectional = True):\n",
        "    super(net, self).__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding.from_pretrained(w2v_weights)\n",
        "\n",
        "    self.frozen_embedding = nn.Embedding.from_pretrained(w2v_weights)\n",
        "    self.frozen_embedding.requires_grad = False\n",
        "\n",
        "    \n",
        "    bi_direction = (2 if bidirectional else 1)\n",
        "\n",
        "    # input_size: dimension size of GloVe embedding dimension.\n",
        "    # Adds 54 to consider the POS words for the paragraph.\n",
        "    self.lstm = nn.GRU(input_size = embedding_dim + 54,\n",
        "                        hidden_size = lstm_hidden_dim, \n",
        "                        bias = True,\n",
        "                        bidirectional = bidirectional,\n",
        "                        dropout = 0.5,\n",
        "                        batch_first = True)\n",
        "    \n",
        "    self.lstm2 = nn.GRU(input_size = embedding_dim,\n",
        "                        hidden_size = lstm_hidden_dim,\n",
        "                        bidirectional = bidirectional,\n",
        "                        dropout = 0.5,\n",
        "                        batch_first = True)\n",
        "    \n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    # Bilear layer as output cause that's what the article says.\n",
        "    self.fc = nn.Bilinear(lstm_hidden_dim * bi_direction , lstm_hidden_dim * bi_direction, 3)\n",
        "\n",
        "  def forward(self,p_w2v_tensor, p_w2v_len, \n",
        "            p_feature_tag, p_feature_tf, \n",
        "            p_match_tensor, \n",
        "            q_w2v_tensor, q_w2v_len):\n",
        "    \n",
        "    #embed_freeze = self.frozen_embedding(p_w2v_tensor)\n",
        "    #embed = self.embedding(p_w2v_tensor)\n",
        "    #q_embed_freeze = self.frozen_embedding(q_w2v_tensor)\n",
        "    #q_embed = self.embedding(q_w2v_tensor)\n",
        "\n",
        "    #combined_embed = self.dropout(torch.cat((embed, embed_freeze), dim=3))\n",
        "    #q_combined_embed = torch.cat((q_embed, q_embed_freeze), dim=2)\n",
        "\n",
        "    combined_embed = self.frozen_embedding(p_w2v_tensor)\n",
        "    q_combined_embed = self.frozen_embedding(q_w2v_tensor)\n",
        "\n",
        "    pos_tensor = torch.zeros(combined_embed.shape[0], combined_embed.shape[1], \n",
        "                             combined_embed.shape[2], 50)\n",
        "    for i, sentence in enumerate(p_feature_tag):\n",
        "      for j, sequence in enumerate(sentence):\n",
        "        for k, word in enumerate(sequence):\n",
        "          pos_tensor[i, j, k, word] = 1\n",
        "\n",
        "    p_feature_tf = p_feature_tf.reshape(p_feature_tf.shape[0], p_feature_tf.shape[1], p_feature_tf.shape[2], 1)\n",
        "    feature = torch.cat((combined_embed, pos_tensor, p_feature_tf, p_match_tensor), dim=3)\n",
        "    logits = torch.full((feature.shape[0], feature.shape[1], feature.shape[2], 3), -1, dtype=float)\n",
        "\n",
        "    for i, paragraph in enumerate(feature):\n",
        "      packed_input = nn.utils.rnn.pack_padded_sequence(paragraph, torch.tensor(p_w2v_len[i]), batch_first=True, enforce_sorted=False)\n",
        "      output, (h_n, c_n) = self.lstm(packed_input)\n",
        "      seq_unpacked, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True, padding_value=-1)\n",
        "\n",
        "      q_feature = q_combined_embed[i]\n",
        "      while q_feature.shape[0] < seq_unpacked.shape[1]:\n",
        "        q_feature = torch.cat((q_feature, torch.full((1, q_feature.shape[1]), -1)))\n",
        "      while seq_unpacked.shape[1] < q_feature.shape[0]:\n",
        "        seq_unpacked = torch.cat((seq_unpacked, torch.full((seq_unpacked.shape[0], 1, seq_unpacked.shape[2]), -1)), dim=1)\n",
        "      seq_len = seq_unpacked.shape[0]\n",
        "      q_feature = q_feature.repeat(seq_len, 1, 1)\n",
        "      q_output, (q_h_n, q_c_n) = self.lstm2(q_feature)\n",
        "\n",
        "      q_output = q_output.contiguous()\n",
        "      seq_unpacked = seq_unpacked.contiguous()\n",
        "      logit_row = self.fc(seq_unpacked, q_output)\n",
        "      for a, sent in enumerate(logit_row):\n",
        "        for b, token in enumerate(sent):\n",
        "          for c, tag in enumerate(token):\n",
        "            logits[i][a][b][c] = logit_row[a][b][c]\n",
        "    return logits\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA3WAiV4kNVr"
      },
      "source": [
        "# **Trainer Class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLsFq18NN0WS"
      },
      "source": [
        "class MultiClassTrainer(object):\n",
        "    \"\"\"\n",
        "    Trainer for training a multi-class classification model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, optimizer, loss_fn, device=\"cpu\", log_every_n=None):\n",
        "        self.model = model.to(device)\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "        self.loss_fn = loss_fn\n",
        "        \n",
        "        self.log_every_n = log_every_n if log_every_n else 0\n",
        "\n",
        "\n",
        "    def _print_summary(self):\n",
        "        print(self.model)\n",
        "        print(self.optimizer)\n",
        "        print(self.loss_fn)\n",
        "\n",
        "    def train(self, loader):\n",
        "        \"\"\"\n",
        "        Run a single epoch of training\n",
        "        \"\"\"\n",
        "\n",
        "        self.model.train() # Run model in training mode\n",
        "\n",
        "        loss_history = []\n",
        "        running_loss = 0.\n",
        "        running_loss_history = []\n",
        "\n",
        "        for i, batch in tqdm(enumerate(loader)):\n",
        "            batch_size = batch[0].shape[0]\n",
        "            self.optimizer.zero_grad() # Always set gradient to 0 before computing it\n",
        "\n",
        "            logits = self.model(batch[0].to(self.device), batch[1], batch[2].to(self.device), batch[3].to(self.device), \n",
        "                                batch[4].to(self.device), batch[5].to(self.device), batch[6]) # __call__ model() in this case: __call__ internally calls forward()\n",
        "            # [batch_size, num_sent, num_classes]\n",
        "            loss = self.loss_fn(logits.view(-1, 3), batch[7].view(-1).to(self.device)) # Compute loss: Cross entropy loss\n",
        "\n",
        "            loss_history.append(loss.item())\n",
        "\n",
        "            running_loss += (loss_history[-1] - running_loss) / (i + 1) # Compute rolling average\n",
        "\n",
        "            loss.backward() # Perform backprop, which will compute dL/dw\n",
        "\n",
        "            if self.log_every_n and i % self.log_every_n == 0:\n",
        "                print(\"Running loss: \", running_loss)\n",
        "\n",
        "                # print(\"Gradients:\")\n",
        "                # for p in list(filter(lambda p: p.grad is not None, self.model.parameters())):\n",
        "                #     print(p.grad.data.norm(2).item())\n",
        "\n",
        "            running_loss_history.append(running_loss)\n",
        "            \n",
        "            nn.utils.clip_grad_norm_(self.model.parameters(), 3.0) # We clip gradient's norm to 3\n",
        "\n",
        "            self.optimizer.step() # Update step: w = w - eta * dL / dW : eta = 1e-2 (0.01), gradient = 5e30; update value of 5e28\n",
        "\n",
        "        print(\"Epoch completed!\")\n",
        "        print(\"Epoch Loss: \", running_loss)\n",
        "        print(\"Epoch Perplexity: \", math.exp(running_loss))\n",
        "\n",
        "        # The history information can allow us to draw a loss plot\n",
        "        return loss_history, running_loss_history\n",
        "\n",
        "    def evaluate(self, loader):\n",
        "        \"\"\"\n",
        "        Evaluate the model on a validation set\n",
        "        \"\"\"\n",
        "\n",
        "        self.model.eval() # Run model in eval mode (disables dropout layer)\n",
        "\n",
        "        batch_wise_true_labels = []\n",
        "        batch_wise_predictions = []\n",
        "\n",
        "        loss_history = []\n",
        "        running_loss = 0.\n",
        "        running_loss_history = []\n",
        "\n",
        "        with torch.no_grad(): # Disable gradient computation - required only during training\n",
        "            for i, batch in tqdm(enumerate(loader)):\n",
        "                # batch[0] shape: (batch_size, input_size)\n",
        "\n",
        "                logits = self.model(batch[0].to(self.device), batch[1], batch[2].to(self.device), batch[3].to(self.device), \n",
        "                                batch[4].to(self.device), batch[5].to(self.device), batch[6]) # __call__ model() in this case: __call__ internally calls forward()\n",
        "                # logits shape: (batch_size, num_classes)\n",
        "                \n",
        "                loss = self.loss_fn(logits.view(-1, 3), batch[7].view(-1).to(self.device)) # Compute loss: Cross entropy loss\n",
        "                # No backprop is done during validation\n",
        "                \n",
        "                # Instead of using CrossEntropyLoss, you use BCEWithLogitsLoss\n",
        "                # BCEWithLogitsLoss - independently calculates loss for each class\n",
        "                \n",
        "\n",
        "                loss_history.append(loss.item())\n",
        "\n",
        "                running_loss += (loss_history[-1] - running_loss) / (i + 1) # Compute rolling average\n",
        "                \n",
        "                running_loss_history.append(running_loss)\n",
        "\n",
        "                # logits : [batch_size, num_classes] and each of the values in logits can be anything (-infinity, +infity)\n",
        "                # Converts the raw outputs into probabilities for each class using softmax\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                # probs shape: (batch_size, num_classes)\n",
        "                # -1 dimension picks the last dimension in the shape of the tensor, in this case 'num_classes'\n",
        "                \n",
        "                predictions = []\n",
        "                for i, paragraph in enumerate(probs):\n",
        "                  predictions.append([])\n",
        "                  max_score, max_index = 0, 0\n",
        "                  start_indices, end_indices = [], []\n",
        "                  for j, sent in enumerate(paragraph[:len(batch[8][i])]):\n",
        "                    max_sent_score, max_start_index, max_end_index = float('-inf'), 0, 0\n",
        "                    sent_len = len(batch[8][i][j])+1\n",
        "                    for k, start_token in enumerate(sent[:sent_len]):\n",
        "                      for l, end_token in enumerate(sent[k:sent_len]):\n",
        "                        p = math.log2(start_token[1]) + math.log2(end_token[2])\n",
        "                        if p > max_sent_score:\n",
        "                          max_sent_score = p\n",
        "                          max_start_index, max_end_index = k, k+l\n",
        "                    start_indices.append(max_start_index)\n",
        "                    end_indices.append(max_end_index)\n",
        "                    if max_sent_score > max_score:\n",
        "                      max_score, max_index = max_sent_score, j\n",
        "                  try:\n",
        "                    predictions[i] = batch[8][i][max_index][start_indices[max_index]-1:end_indices[max_index]]\n",
        "                  except IndexError:\n",
        "                    predictions[i] = \"\"\n",
        "\n",
        "                batch_wise_true_labels.append(batch[10])\n",
        "                batch_wise_predictions.append(predictions)\n",
        "        \n",
        "        all_true_labels = list(chain.from_iterable(batch_wise_true_labels))\n",
        "        all_predictions = list(chain.from_iterable(batch_wise_predictions))\n",
        "        precision, recall, f1 = MultiClassTrainer.compute_f1(all_true_labels, all_predictions)\n",
        "\n",
        "        # Now we can generate a classification report\n",
        "        print(\"Classification report after epoch:\")\n",
        "        print(\"Precision: {}, Recall: {}, F-1: {}\".format(precision, recall, f1))\n",
        "\n",
        "        return loss_history, running_loss_history\n",
        "\n",
        "    def baseline_evaluate(self, loader):\n",
        "        \"\"\"\n",
        "        Evaluate the model on a validation set\n",
        "        \"\"\"\n",
        "\n",
        "        batch_wise_true_labels = []\n",
        "        batch_wise_predictions = []\n",
        "\n",
        "        loss_history = []\n",
        "        running_loss = 0.\n",
        "        running_loss_history = []\n",
        "\n",
        "        with torch.no_grad(): # Disable gradient computation - required only during training\n",
        "            for i, batch in tqdm(enumerate(loader)):\n",
        "                # batch[0] shape: (batch_size, input_size)\n",
        "\n",
        "                predictions = []\n",
        "                for i, paragraph in enumerate(batch[8]):\n",
        "                  predictions.append([])\n",
        "                  for j, sent in enumerate(paragraph):\n",
        "                    predictions[i] += sent\n",
        "\n",
        "                batch_wise_true_labels.append(batch[10])\n",
        "                batch_wise_predictions.append(predictions)\n",
        "        \n",
        "        all_true_labels = list(chain.from_iterable(batch_wise_true_labels))\n",
        "        all_predictions = list(chain.from_iterable(batch_wise_predictions))\n",
        "        precision, recall, f1 = MultiClassTrainer.compute_f1(all_true_labels, all_predictions)\n",
        "\n",
        "        # Now we can generate a classification report\n",
        "        print(\"Classification report after epoch:\")\n",
        "        print(\"Precision: {}, Recall: {}, F-1: {}\".format(precision, recall, f1))\n",
        "\n",
        "        return loss_history, running_loss_history\n",
        "    \n",
        "    @staticmethod\n",
        "    def compute_f1(a_gold, a_pred):\n",
        "      def normalize_answer(s):\n",
        "        \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "        def remove_articles(text):\n",
        "          regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
        "          return re.sub(regex, ' ', text)\n",
        "        def white_space_fix(text):\n",
        "          return ' '.join(text.split())\n",
        "        def remove_punc(text):\n",
        "          exclude = set(string.punctuation)\n",
        "          return ''.join(ch for ch in text if ch not in exclude)\n",
        "        def lower(text):\n",
        "          return text.lower()\n",
        "        return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "      \n",
        "      def get_tokens(s):\n",
        "        if not s: return []\n",
        "        return normalize_answer(s).split()\n",
        "      \n",
        "      total_precision, total_recall, total_f1 = 0, 0, 0\n",
        "      for g, p in zip(a_gold, a_pred):\n",
        "        gold_toks = get_tokens(\" \".join(g))\n",
        "        pred_toks = get_tokens(\" \".join(p))\n",
        "        common = Counter(gold_toks) & Counter(pred_toks)\n",
        "        num_same = sum(common.values())\n",
        "        if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "          precision = recall = f1 = int(gold_toks == pred_toks)\n",
        "        else:\n",
        "          precision = 1.0 * num_same / len(pred_toks)\n",
        "          recall = 1.0 * num_same / len(gold_toks)\n",
        "          f1 = 0 if (precision == 0 and recall == 0) else (2 * precision * recall) / (precision + recall)\n",
        "        total_precision += precision\n",
        "        total_recall += recall\n",
        "        total_f1 += f1\n",
        "      return (total_precision/len(a_gold), total_recall/len(a_gold), total_f1/len(a_gold))\n",
        "\n",
        "\n",
        "    def get_model_dict(self):\n",
        "        return self.model.state_dict()\n",
        "\n",
        "    def run_training(self, train_loader, valid_loader, n_epochs=10):\n",
        "        # Useful for us to review what experiment we're running\n",
        "        # Normally, you'd want to save this to a file\n",
        "        self._print_summary()\n",
        "\n",
        "        train_losses = []\n",
        "        train_running_losses = []\n",
        "\n",
        "        valid_losses = []\n",
        "        valid_running_losses = []\n",
        "\n",
        "        for i in range(n_epochs):\n",
        "            loss_history, running_loss_history = self.train(train_loader[i])\n",
        "            valid_loss_history, valid_running_loss_history = self.evaluate(valid_loader[i])\n",
        "\n",
        "            train_losses.append(loss_history)\n",
        "            train_running_losses.append(running_loss_history)\n",
        "\n",
        "            valid_losses.append(valid_loss_history)\n",
        "            valid_running_losses.append(valid_running_loss_history)\n",
        "\n",
        "        # Training done, let's look at the loss curves\n",
        "        all_train_losses = list(chain.from_iterable(train_losses))\n",
        "        all_train_running_losses = list(chain.from_iterable(train_running_losses))\n",
        "\n",
        "        all_valid_losses = list(chain.from_iterable(valid_losses))\n",
        "        all_valid_running_losses = list(chain.from_iterable(valid_running_losses))\n",
        "\n",
        "        train_epoch_idx = range(len(all_train_losses))\n",
        "        valid_epoch_idx = range(len(all_valid_losses))\n",
        "        # sns.lineplot(epoch_idx, all_losses)\n",
        "        sns.lineplot(train_epoch_idx, all_train_running_losses)\n",
        "\n",
        "        sns.lineplot(valid_epoch_idx, all_valid_running_losses)\n",
        "        plt.show()\n",
        "\n",
        "    def baseline_run_training(self, train_loader, valid_loader, n_epochs=10):\n",
        "        # Useful for us to review what experiment we're running\n",
        "        # Normally, you'd want to save this to a file\n",
        "        self._print_summary()\n",
        "\n",
        "        for i in range(n_epochs):\n",
        "            valid_loss_history, valid_running_loss_history = self.baseline_evaluate(valid_loader[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1IM_kTwkU_h"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38iQrfeSii5V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475,
          "referenced_widgets": [
            "b06d23be5fae4de09966c816059f6dc8",
            "1d704732f5214405a6e4f324579c891a",
            "96d8468f164c4b629cf0a44d90b19cfb",
            "1add8fd6ebca457fb90dc2e05c5bde0c",
            "58cea2731a7c4a81b499c4c1ffe96c38",
            "28ccd2da5581439a99558dc916c9e8be",
            "0401f954d1784713bfe80ef890faf6be",
            "dd967246509145dfaa4797b407b1c821",
            "ee4980a87d934e68aa47420277ba96fc",
            "4c567bb55ba947b7a536d64801e4f62f",
            "72d9d71730e640828f9405663c57c6af",
            "967d6bfb673e4acfb73088cf2008181c",
            "209e4d5170174ca1ab5498788fda2083",
            "027640706660429a8ff2bdac11fb03a6",
            "33a207a4fabc4bbd9ef01b51327f1bef",
            "47f45e74dfd8437888680af94bc243b5"
          ]
        },
        "outputId": "9901c9d4-7245-4359-d12d-727f5f664757"
      },
      "source": [
        "\n",
        "model = net(w2v_weights=torch.FloatTensor(w2v_weights.vectors), embedding_dim=100)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=-1, weight=torch.DoubleTensor([1, 50, 50]))\n",
        "trainer = MultiClassTrainer(model, optimizer, loss_fn, log_every_n=10)\n",
        "trainer.baseline_run_training(train_loader, valid_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "net(\n",
            "  (embedding): Embedding(400004, 100)\n",
            "  (frozen_embedding): Embedding(400004, 100)\n",
            "  (lstm): GRU(154, 100, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "  (lstm2): GRU(100, 100, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Bilinear(in1_features=200, in2_features=200, out_features=3, bias=True)\n",
            ")\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.01\n",
            "    weight_decay: 0\n",
            ")\n",
            "CrossEntropyLoss()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b06d23be5fae4de09966c816059f6dc8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Classification report after epoch:\n",
            "Precision: 0.030716225382990466, Recall: 0.9946056047989973, F-1: 0.05773136959165499\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee4980a87d934e68aa47420277ba96fc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}